## Table of Contents

1.  Under the Hood of Random Forest

2.  Classification Case: Titanic Survival Prediction

3.  Regression Case: Miles Per Gallon Prediction

## Under the Hood of Random Forest

This section of our vignette is for those who want to understand the inner workings of random forest algorithms. Note that random forest is an ensemble method, i.e., it combines the results of multiple decision trees, so before we go over a high-level overview of the random forest algorithm, we will first learn about some intuition behind decision trees.

This section will go over the following:

1.  Intuition Behind Decision Trees

2.  Overview of the Random Forest Algorithm

### Intuition Behind Decision Trees

Note that we will be explaining decision trees in the context of solving a classification problem. A decision tree is a rule-based algorithm that systematically divides the predictor space, i.e., the target variable using a set of rules to split the data points into homogeneous groups. Inner nodes and root nodes represent when a rule is applied to split a predictor (bearing in mind that a decision tree follows a binary tree structure). One branch of a node contains the data points which satisfy the node's rule, while the other branch contains the data points which broke the rule.

The goal is to split the predictor space, i.e., the target variable into increasingly homogeneous subgroups compared to its parent node. This process continues until either no more rules can be applied or there are no remaining data points. The nodes found at the bottom of the decision tree after the splitting process is over are called terminal or leaf nodes.

The decision tree's algorithm attempts to split the data into leaf nodes which contain only a single class. These nodes are referred to as pure nodes. Not all the leaf nodes of a decision tree will be completely pure, i.e., some leaf nodes will contain a mix of multiple classes. In this case, a classification is made based on the most common data point in a node.

**How does a decision tree decide how to split?**

Let us explain this using an example. Imagine we want to predict a students exam result based off whether they are taking online classes, student background, and working status. To establish the root node, the decision tree algorithm will iterate through splitting each predictor to determine which split results in the most homogenous or pure nodes, and it will evaluate this using some statistical criterion. After the root node is established, subsequent splits in the tree consider only the predictors that haven't been used for splitting in the current branch.

Variable selection criterion is done using one of the following approaches:

• Entropy and information gain

• Gini index

It is left to the reader to look further into entropy and information gain, but for the purpose of this vignette, we will only explain the use of the Gini index.

Let the following be our toy data set for this example.

![Toy Data set for Decision Tree Example](images/toy-dataset2.png)

This is the formula for calculating Gini index:

![Gini Index Formula](images/gini-index-formula.png){width="386"}

Keep in mind that $j$ denotes the number of classes, and $p_j$ signifies the proportion of data points belonging to class $j$ within the current node.

Splitting by student background, we get three possible child nodes: maths, CS, and others.

![Student Background Split Condition](images/bkgrd-tree-ex.jpg)

Lets us calculate the Gini index of the child nodes of Student Background.

Maths node: 2P, 5F

$$
Gini_{maths} = 1 - (\frac{2}{7})^2 - (\frac{5}{7})^2 = .4082
$$

CS node: 4P, 0F

$$
Gini_{CS} = 1 - (\frac{4}{4})^2 - (\frac{0}{4})^2 = 0
$$

Other node: 2P, 2F

$$
Gini_{other} = 1 - (\frac{2}{4})^2 - (\frac{2}{4})^2 = .5
$$

The overall Gini index of this split is calculated by taking the weighted average of the 3 nodes.

$$
Gini_{bkgrd} = \frac{7}{15}(.4082) + \frac{4}{15}(0) + \frac{4}{15}(.5) = .3238
$$

Similarly, we will calculate the Gini index for 'Work Status' and 'Online Courses' predictors.

$$
Gini_{working} = 1 - (\frac{6}{9})^2 - (\frac{3}{9})^2 = .4444
$$

$$
Gini_{not working} = 1 - (\frac{4}{6})^2 - (\frac{2}{6})^2 = .4444
$$

$$
Gini_{workstatus} = \frac{6}{15}(.4444) + \frac{9}{15}(.4444) = .4444
$$

$$
Gini_{online} = 1 - (\frac{4}{8})^2 - (\frac{4}{8})^2 = .5
$$

$$
Gini_{notonline} = 1 - (\frac{3}{7})^2 - (\frac{4}{7})^2 = .4898
$$

$$
Gini_{onlinecourse} = \frac{7}{15}(.4898) + \frac{8}{15}(.5) = .49524
$$

Since the Gini index is lowest for 'Student Background,' this predictor becomes the basis for splitting the root node. This concludes the logic behind how the split conditions for decision nodes are created.

### Overview of Random Forest Algorithm

The random forest algorithm follows these steps:

1\. Take the original dataset and create $N$ bootstrapped samples of size $n$ such that $n$ is smaller than the size of the original dataset.

2\. Train a decision tree for each of the bootstrapped samples, but split on a different subset of the predictors for each tree and determine the best split using impurity measures such as Gini impurity or Entropy.

3\. Create a prediction by aggregating the results of all the trees. In the classification case, take the majority vote across all trees, and in the regression case, take the average across all trees.

## Classification Case: Titanic Survival Prediction

### Prerequisites

Copy and paste the following block of code into a new script to load the required packages and data used for this example. If an error appears, then you likely don't have one of the libraries installed.

```{r, message=FALSE, warning=FALSE}
library(ISLR)
library(ISLR2)
library(tidyverse)
library(tidymodels)
library(forcats)
library(ggthemes)
library(naniar)
library(corrplot)
library(corrr)
library(klaR)
library(ggplot2)

tidymodels_prefer()

titanic_data <- read.csv('../data/titanic.csv')

titanic_data_1 <- titanic_data %>% 
  mutate(survived = factor(survived, levels = c("Yes", "No"))) %>% 
  mutate(pclass = factor(pclass))
```

### Partition

In the process of model construction, a crucial step involves partitioning the data into training and testing sets. The model originally learns patterns from the training set, while the testing set serves as a benchmark to test the performance of the model on unseen data.

The `initial_split(titanic_data_1, strata = survived, prop = 0.7)` function is used to allocate 70% of the titanic data set into a training set and allocating the other 30% into a testing set while stratifying by the `survived` variable.

Note that `training(parition)` and `testing(parition)` are used to retrieve the training and testing sets, respectively.

```{r}
set.seed(3435)
parition <- initial_split(titanic_data_1, strata = survived, prop = 0.7)
train_set <- training(parition)
test_set <- testing(parition)
```

Note that we need to stratify by survived because there is an uneven proportion of people who survived the titanic versus those who did not.

```{r, echo=FALSE}
train_set %>%
  ggplot(aes(x = factor(survived), fill = factor(survived))) +
  geom_bar(color = "black", show.legend = FALSE) +
  labs(title = "Distribution of Survival in Training Set",
       x = "Survived",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, vjust = 0.5, hjust = 1))
```

### K-Folds Cross Validation

K-fold cross validation allows us to train and evaluate the performance of our model on $k$ different partitions of the training set, reducing the risk of over fitting. The `vfold_cv(train_set, v = 5, strata = "survived")` function will create 5 training folds of our training set while stratifying by the `survived` variable. It is left to the reader to look further into k-fold cross validation.

```{r}
train_folds <- vfold_cv(train_set, v = 5, strata = "survived")
```

### Data Preparation

We will preprocess our data using a recipe from tidymodels. Building a recipe will allow us to provide instructions for preparing and transforming the data before using it to train our model.

The `recipe()` function will initialize the creation of a recipe, setting `survived` as the target variable and `pclass`, `sex`, `age`, `sib_sp`, `parch`, and `fare` as predictors.

```{r}
train.recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, data = train_set) %>% 
  step_impute_linear(age,
                     impute_with = imp_vars(fare)) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact(terms = ~ starts_with('sex'):fare + age:fare)
```

Note that there is some missing data in the `age` variable, using `step_impute_linear(age, impute_with = imp_vars(fare))` we will impute missing values of age with linear regression, using `fare` as a predictor.

```{r, echo=FALSE}
vis_miss(train_set)
```

\[ Explain use of interaction terms\]

```{r, echo=FALSE}
train_set %>% 
  select(where(is.numeric)) %>% 
  cor(use = 'pairwise.complete.obs') %>% 
  corrplot(type = 'lower', diag = FALSE,
           method = 'color')
```

The `step_dummy(all_nominal_predictors())` function simply turns all the nominal predictors into dummies.

### Model Fitting

The first step is to specify the model's hyper parameters, engine, and mode.

```{r}
rf_class_spec <- rand_forest(mtry = tune(),
                             trees = tune(),
                             min_n = tune()) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")
```

The `rand_forest(mtry = tune(), trees = tune(), min_n = tune())` function is used to specify the hyper parameters being tuned.

-   `mtry`: Number of predictors used to train each decision tree in the random forest, e.g, if we want to build decision trees that were each trained on 3 random predictors of the original training set, then we would set `mtry = 3`.

-   `trees`: Number of decision trees to be included in the random forest, e.g., if we only wanted our forest to contain 4 decision trees, then we would set `trees = 4`.

-   `min_n`: Minimum number of observations required for further splitting, e.g., if the number of observations within a node falls below this threshold during the tree-building process, then further splitting of this node is halted and it becomes a terminal node.

The `set_engine("ranger", importance = "impurity")` function allows us to use the random forest implementation from the "ranger" package, and it set the importance method as "impurity". Note that variable importance is the measure of the contribution that each predictor makes to the predictive performance of the model. The decision trees of a random forest are split on different subsets of predictors, and the impurity method calculates importance by measuring how much each predictor is involved in reducing impurity across all trees.

The `set_mode("classification")` function simply specifies the model as a classification model.

Next, we simply need to define a model building workflow which is going to combine our random forest classification model (`rf_class_spec`) and our data preparation recipe (`train.recipe`).

```{r}
rf_class_wf <- workflow() %>% 
  add_model(rf_class_spec) %>% 
  add_recipe(train.recipe)
```

Afterwards, we will set up a tuning grid with the `grid_regular()` function to experiment with different variations of the hyper parameters within their defined ranges and apply the tuning grid to the `tune_grid()` function to find the most optimal configuration.

```{r}
rf_grid <- grid_regular(mtry(range = c(1, 3)),
                        trees(range = c(200, 600)),
                        min_n(range = c(10, 20)),
                        levels = 8)


#tuning the random forest model

tune_rf <- tune_grid(
  rf_class_wf,
  resamples = train_folds,
  grid = rf_grid
)
```

Finally, we will extract the best configuration of the hyper parameters using the **`select_best()`** function, create a finalized version of the random forest workflow with **`finalize_workflow()`** , and fit the finalized model using the entire training set with **`fit()`**.

```{r}
best_rf <- select_best(tune_rf)

rf_final <- finalize_workflow(rf_class_wf, best_rf)

rf_final_fit <- fit(rf_final, data = train_set)
```

### Prediction

To make predictions using the trained random forest model (`rf_final_fit`), use the `predict()` function and provide it with new input data (`new_data`). It is required that the structure of the input data aligns with the predictor variables used during the model training process.

```{r}
new_data <- test_set[c("pclass", "sex", "age", "sib_sp", "parch", "fare")]
predict(rf_final_fit, new_data)
```

### Accuracy Measures

\[ Explain accuracy measures and model results \]

```{r}
augment(rf_final_fit, new_data = test_set) %>% 
  roc_auc(survived, .pred_Yes)

augment(rf_final_fit, new_data = test_set) %>% 
  sensitivity(survived, .pred_class)

augment(rf_final_fit, new_data = test_set) %>% 
  specificity(survived, .pred_class)

augment(rf_final_fit, new_data = test_set) %>% 
  accuracy(survived, .pred_class)
```

## \[ Add Regression Case \]
